import copy
from collections import Counter
from collections.abc import Iterable

import torch.utils.data
import torchvision.datasets as datasets
import torchvision.transforms as transforms

from dataloaders.base import BaseLoader


class Cifar10Loader(BaseLoader):
    def __init__(self, batch_size=128):
        super(Cifar10Loader, self).__init__()
        mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)
        self.train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(15),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])

        self.valid_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])

        self.set_default_loader(batch_size)
        self._original_train_set = copy.deepcopy(self.train_set)
        self._original_valid_set = copy.deepcopy(self.valid_set)
        self.batch_size = batch_size

    def set_default_loader(self, batch_size=None):
        self.batch_size = self.batch_size if batch_size is None else batch_size

        self.train_set = datasets.CIFAR10("./data", train=True, transform=self.train_transform, download=True)
        self.valid_set = datasets.CIFAR10("./data", train=False, transform=self.valid_transform)

        self.train_loader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)
        self.valid_loader = torch.utils.data.DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False)

        self.train_iterations = len(self.train_loader)
        self.valid_iterations = len(self.valid_loader)

    def set_subclass_loader(self, sub_classes, batch_size=None, dustbin_class_ratio=1.):
        """
        :param sub_classes: iterable, wanted classes to be generated by loader
        :param batch_size: int, batch size of loader
        :param dustbin_class_ratio: float, dustbin classes ratio,
        1.0 means the number of dustbin classes are same to those of sub classes (# of dustbin == # of sub classes)
        0.0 means the loader have only sub classes set
        :return: None
        """
        assert isinstance(sub_classes, Iterable), "sub_classes must to be iterable e.g. [1] or [3,5,8]"
        self.batch_size = self.batch_size if batch_size is None else batch_size
        sub_classes = sorted(sub_classes)
        num_subclasses, num_dustbin = len(sub_classes), (len(self.train_set.classes) - len(sub_classes))
        dic_class_cnt = Counter(self.train_set.targets)  # class당 데이터 갯수
        n_sample_sub_classes, n_sample_dustbin = 0, 0    # sub class 총 데이터 갯수 & dustbin class 총 데이터 갯수
        for i in range(len(self.train_set.classes)):
            if i in sub_classes:
                n_sample_sub_classes += dic_class_cnt[i]
            else:
                n_sample_dustbin += dic_class_cnt[i]
        # train set
        # selecting samples
        new_samples, new_targets = [], []
        idx = 0
        while idx < len(self.train_set.targets):
            target = self.train_set.targets[idx]
            if target in sub_classes:   # sub class의 데이터는 전부 뽑음
                n_samples_per_class = dic_class_cnt[target]
                new_samples.extend(self.train_set.data[idx:idx + n_samples_per_class])
                new_targets.extend(self.train_set.targets[idx:idx + n_samples_per_class])
            else:   # dustbin class당 데이터 갯수: 전체 subclass 데이터의 수를 dustbin 클래스의 수로 나눠줌
                n_samples_per_class = dustbin_class_ratio * (n_sample_sub_classes // num_dustbin)
                new_samples.extend(self.train_set.data[idx:idx + n_samples_per_class])
                new_targets.extend(self.train_set.targets[idx:idx + n_samples_per_class])
            idx += dic_class_cnt[target]

        idx_to_new_idx = {c: i + 1 for i, c in enumerate(sub_classes)}  # mapping to new target idx / 0: others
        # re-labeling
        for i, (sample, target) in enumerate(zip(new_samples, new_targets)):
            if target in sub_classes:
                new_label = idx_to_new_idx[target]
                new_samples[i] = (sample[0], new_label)
                new_targets[i] = new_label
            else:
                new_label = 0
                new_samples[i] = (sample[0], new_label)
                new_targets[i] = new_label

        # valid set
        # selecting samples
        dic_class_cnt = Counter(self.valid_set.targets)
        new_samples, new_targets = [], []
        idx = 0
        while idx < len(self.valid_set.targets):
            target = self.valid_set.targets[idx]
            if target in sub_classes:
                n_samples_per_class = 50
                new_samples.extend(self.valid_set.samples[idx:idx + n_samples_per_class])
                new_targets.extend(self.valid_set.targets[idx:idx + n_samples_per_class])
            else:
                n_samples_per_class = 1
                new_samples.extend(self.valid_set.samples[idx:idx + n_samples_per_class])
                new_targets.extend(self.valid_set.targets[idx:idx + n_samples_per_class])
            idx += dic_class_cnt[target]

        idx_to_new_idx = {c: i + 1 for i, c in enumerate(sub_classes)}  # mapping to new target idx / 0: others
        # re-labeling
        for i, (sample, target) in enumerate(zip(new_samples, new_targets)):
            if target in sub_classes:
                new_label = idx_to_new_idx[target]
                new_samples[i] = (sample[0], new_label)
                new_targets[i] = new_label
            else:
                new_label = 0
                new_samples[i] = (sample[0], new_label)
                new_targets[i] = new_label



    def set_subclass_loader(self, sub_classes, batch_size=None):
        """make loader having subclass"""
        assert isinstance(sub_classes, Iterable), "sub_classes must to be iterable e.g. [1] or [3,5,8]"
        self.batch_size = self.batch_size if batch_size is None else batch_size

        sub_idx = []
        for idx, classes in enumerate(self.train_set.targets):
            if classes in sub_classes:
                sub_idx.append(idx)

        self.train_set = copy.deepcopy(self._original_train_set)
        self.train_set.data = self.train_set.data[sub_idx]
        self.train_set.targets = [self.train_set.targets[i] for i in sub_idx]

        sub_idx = []
        for idx, classes in enumerate(self.valid_set.targets):
            if classes in sub_classes:
                sub_idx.append(idx)

        self.valid_set = copy.deepcopy(self._original_valid_set)
        self.valid_set.data = self.valid_set.data[sub_idx]
        self.valid_set.targets = [self.valid_set.targets[i] for i in sub_idx]

        self.train_loader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)
        self.valid_loader = torch.utils.data.DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False)

        self.train_iterations = len(self.train_loader)
        self.valid_iterations = len(self.valid_loader)
